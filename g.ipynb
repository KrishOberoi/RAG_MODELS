{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxAtomnDXXujuACim0Olii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrishOberoi/RAG_MODELS/blob/main/g.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjoGhmr6AlRm"
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-hub\n",
        "!pip install -q arxiv\n",
        "!pip install -q semanticscholar\n",
        "!pip install -q sentence-transformers==2.3.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ragatouille\n",
        "!pip install -q llama-index-readers-file"
      ],
      "metadata": {
        "id": "r95haiu0AwAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HHl-ouESPJZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from llama_index.readers.file import PDFReader\n",
        "\n",
        "# Step 1: Upload multiple PDF files\n",
        "uploaded = files.upload()  # allows you to select and upload multiple files\n",
        "\n",
        "# Step 2: Initialize PDFReader\n",
        "loader = PDFReader()\n",
        "\n",
        "# Step 3: Load data from all uploaded PDFs\n",
        "documents = []\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Loading {filename}...\")\n",
        "    docs = loader.load_data(filename)\n",
        "    documents.extend(docs)\n",
        "\n",
        "print(f\"âœ… Loaded {len(documents)} documents from {len(uploaded)} PDFs.\")\n"
      ],
      "metadata": {
        "id": "MccyjWxvAx98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_pdf_documents = [document.text for document in documents]\n"
      ],
      "metadata": {
        "id": "sfQf7ApAA1E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monkey patch transformers.AdamW if it's missing\n",
        "import transformers\n",
        "import torch.optim\n",
        "\n",
        "if not hasattr(transformers, \"AdamW\"):\n",
        "    transformers.AdamW = torch.optim.AdamW\n",
        "\n",
        "# Now import RAGatouille\n",
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "# Load ColBERTv2\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n"
      ],
      "metadata": {
        "id": "T_uMqmy2SGQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_overlap(text, chunk_size=400, overlap=50):\n",
        "    assert chunk_size > overlap, \"Chunk size must be greater than overlap\"\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    text_len = len(text)\n",
        "\n",
        "    while start < text_len:\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap  # move forward with overlap\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Usage example for all docs:\n",
        "processed_documents = []\n",
        "for doc in list_pdf_documents:\n",
        "    chunks = split_text_overlap(doc, chunk_size=400, overlap=50)\n",
        "    processed_documents.extend(chunks)\n",
        "\n",
        "# Now index with RAG (disable automatic splitting)\n",
        "RAG.index(\n",
        "    collection=processed_documents,\n",
        "    index_name=\"constitution_index\",\n",
        "    split_documents=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "pEJvS6sdD5Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = RAG.search(query=\"diary entries of august 1942\", k=15, index_name=\"constitution_index\")\n"
      ],
      "metadata": {
        "id": "QkyrxyN2EA_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "CHUgF4mIS8Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc, in enumerate(results):\n",
        "    print(f\"---------------------------------- doc-{i} ------------------------------------\")\n",
        "    print(doc[\"content\"])\n"
      ],
      "metadata": {
        "id": "9pEnOuf3S9X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: feed the output chunks im getting to an gemini api LLM which gives me a response based on the chunks which are fed to it strictly no hallucinations hence making it a strict RAG implementation\n",
        "\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY = 'AIzaSyClBgxT6W5fa2c2CDadiJN5EfPwfKajXjk'\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "def get_rag_response(query, search_results):\n",
        "    \"\"\"\n",
        "    Feeds search results (chunks) to the LLM to answer the query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "        search_results (list): A list of search results from RAGatouille,\n",
        "                                where each item has a 'content' key.\n",
        "\n",
        "    Returns:\n",
        "        str: The response from the LLM based on the provided chunks.\n",
        "    \"\"\"\n",
        "    if not search_results:\n",
        "        return \"No relevant information found in the documents.\"\n",
        "\n",
        "\n",
        "    context = \"\\n\\n\".join([result['content'] for result in search_results])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "   You are a highly detailed and comprehensive knowledge retrieval system.\n",
        "Using ONLY the following information, answer the user's query.\n",
        "Do NOT use any external knowledge. If the information provided does not contain the answer,\n",
        "state that you cannot answer based on the provided context.\n",
        "\n",
        "**Instructions for Detailed Responses:**\n",
        "If the user's query explicitly asks for an **explanation, summary, \"tell me more,\" \"what is,\" \"describe,\" or \"elaborate\"** on a topic, then:\n",
        "1.  Provide a **thorough and detailed answer** by synthesizing **ALL relevant points from ALL provided chunks**.\n",
        "2.  **Expand on each concept and clause present in the provided information**, breaking down its meaning as extensively as possible *using only the words and implications found within the given context*.\n",
        "3.  **Do NOT introduce any external knowledge or interpretations not directly stated or clearly implied by the provided text.**\n",
        "4.  If the provided information is very brief and does not offer sufficient detail for a comprehensive explanation, state that you can only explain it based on the limited information provided, and then proceed to expand as much as possible *from that limited text*.\n",
        "\n",
        "    Information:\n",
        "    {context}\n",
        "\n",
        "    User Query:\n",
        "    {query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during LLM generation: {e}\")\n",
        "        return \"An error occurred while generating the response.\"\n",
        "\n",
        "query = 'diary entries of august 1942 explain in detail'\n",
        "llm_response = get_rag_response(query, results)\n",
        "print(\"\\n--- LLM Response ---\")\n",
        "llm_response\n",
        "\n"
      ],
      "metadata": {
        "id": "SG3o_28eDKhD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gk9CcR0ELrOS"
      }
    }
  ]
}